#FROM ghcr.io/ggml-org/llama.cpp:server-cuda
FROM docker.io/vllm/vllm-openai:latest

#          -DCMAKE_LINKER_TYPE=MOLD 
# RUN \
#     curl -Ls -o /tmp/cmake.sh https://github.com/Kitware/CMake/releases/download/v4.0.2/cmake-4.0.2-linux-x86_64.sh \
#     && bash /tmp/cmake.sh --prefix=/usr/local --skip-license \
#     && rm /tmp/cmake.sh \
#     && curl -Ls https://github.com/rui314/mold/releases/download/v2.39.0/mold-2.39.0-x86_64-linux.tar.gz \
#        | tar xz -C /usr/local --strip-components=1 --no-same-owner \
#     && which cmake \
#     && cmake --version \
#     && which mold \
#     && mold --version

# install llama.cpp
RUN \
    --mount=type=cache,target=/root/.cache,sharing=locked \
    apt-get update --quiet && apt-get install --assume-yes libcurl4-openssl-dev ccache inotify-tools \
    && git clone --recursive --depth 1 --branch master https://github.com/ggml-org/llama.cpp /opt/llama.cpp \
    && cmake \
         -DGGML_RPC=ON \
         -DGGML_CUDA=ON \
         -DGGML_CUDA_FA_ALL_QUANTS=ON \
         -DGGML_CUDA_FORCE_MMQ=ON \
         -DGGML_SCHED_MAX_COPIES=1 \
         -DCMAKE_CUDA_ARCHITECTURES="86" \
         -DLLAMA_CURL=ON \
         -DCMAKE_CUDA_COMPILER_LAUNCHER="ccache" \
         -DCMAKE_C_COMPILER_LAUNCHER="ccache" \
         -DCMAKE_CXX_COMPILER_LAUNCHER="ccache" \
         -S /opt/llama.cpp \
         -B /opt/llama.cpp/build \
    && cmake --build /opt/llama.cpp/build --config Release --parallel

# install exllamav2
RUN \
    --mount=type=cache,target=/root/.cache,sharing=locked \
    git clone https://github.com/turboderp/exllamav2 /opt/exllamav2 \
    && cd /opt/exllamav2 \
    && env MAX_JOBS=4 TORCH_CUDA_ARCH_LIST="8.6" python3 -m pip install flash-attn --no-build-isolation \
    && TORCH_CUDA_ARCH_LIST="8.6" python3 -m pip install -e .

#python setup.py build_ext -i \
#    && pip install -e .

RUN \
    --mount=type=cache,target=/root/.cache,sharing=locked \
    git clone https://github.com/theroyallab/tabbyAPI /opt/tabbyAPI \
    && cd /opt/tabbyAPI \
    && python3 -m pip install -e .

# RUN curl -Ls https://github.com/mostlygeek/llama-swap/releases/download/v100/llama-swap_100_linux_amd64.tar.gz \
#     | tar xz -C /usr/local/bin

RUN \
    --mount=type=cache,target=/root/.cache,sharing=locked \
    curl -Ls https://go.dev/dl/go1.24.2.linux-amd64.tar.gz | tar xz -C /usr/local \
    && cp -s /usr/local/go/bin/* /usr/local/bin \
    && cd /tmp \
    && git clone https://github.com/mostlygeek/llama-swap \
    && cd llama-swap \
    && git checkout 5025c2f1f312703e6810457a20070e5681b264b5 \
    && make clean all \
    && cp /tmp/llama-swap/build/llama-swap-linux-amd64 /usr/local/bin/llama-swap \
    && go install github.com/go-delve/delve/cmd/dlv@latest \
    && ln -s /root/go/bin/dlv /usr/local/bin/dlv






# ENV PIP_ROOT_USER_ACTION=ignore
# RUN python3 -m pip install flash-attn qwen-vl-utils[decord] torchvision fastapi uvicorn torch pillow gputil 
