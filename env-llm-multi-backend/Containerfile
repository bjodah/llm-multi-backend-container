#FROM ghcr.io/ggml-org/llama.cpp:server-cuda
FROM docker.io/vllm/vllm-openai:latest

ARG TORCH_CUDA_ARCH_LIST="8.6"
ARG CUDA_ARCHITECTURES="86"

#          -DCMAKE_LINKER_TYPE=MOLD 
# RUN \
#     curl -Ls -o /tmp/cmake.sh https://github.com/Kitware/CMake/releases/download/v4.0.2/cmake-4.0.2-linux-x86_64.sh \
#     && bash /tmp/cmake.sh --prefix=/usr/local --skip-license \
#     && rm /tmp/cmake.sh \
#     && curl -Ls https://github.com/rui314/mold/releases/download/v2.39.0/mold-2.39.0-x86_64-linux.tar.gz \
#        | tar xz -C /usr/local --strip-components=1 --no-same-owner \
#     && which cmake \
#     && cmake --version \
#     && which mold \
#     && mold --version

# install llama.cpp
RUN \
    --mount=type=cache,target=/root/.cache,sharing=locked \
    apt-get update --quiet && apt-get install --assume-yes libcurl4-openssl-dev ccache inotify-tools \
    && git clone --recursive --depth 1 --branch master https://github.com/ggml-org/llama.cpp /opt/llama.cpp \
    && cmake \
         -DLLAMA_CURL=ON \
         -DGGML_RPC=ON \
         -DGGML_CUDA=ON \
         -DGGML_CUDA_FA_ALL_QUANTS=ON \
         -DGGML_SCHED_MAX_COPIES=1 \
         -DGGML_CUDA_FORCE_MMQ=ON \
         -DCMAKE_CUDA_ARCHITECTURES="${CUDA_ARCHITECTURES}" \
         -DCMAKE_CUDA_COMPILER_LAUNCHER="ccache" \
         -DCMAKE_C_COMPILER_LAUNCHER="ccache" \
         -DCMAKE_CXX_COMPILER_LAUNCHER="ccache" \
         -S /opt/llama.cpp \
         -B /opt/llama.cpp/build \
    && cmake --build /opt/llama.cpp/build --config Release --parallel

ENV TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST}

# MAX_JOBS=1 because OOM crashes whole system... (even then usage peaks around 30GB...)
RUN \
    --mount=type=cache,target=/root/.ccache,sharing=locked \
    --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    --mount=type=cache,target=/root/.cache/huggingface,sharing=locked \
    python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128 \
    && python3 -m pip install --no-build-isolation \
       pandas ninja fastparquet safetensors pygments websockets regex 'numpy<2.3' rich pillow pytest \
    && git clone --branch v2.8.0.post2 https://github.com/Dao-AILab/flash-attention /opt/flash-attention \
    && cd /opt/flash-attention \
    && bash -c 'ulimit -v 42000000; env NVCC_THREADS=1 MAX_JOBS=1 FLASH_ATTN_CUDA_ARCHS=80 PYTORCH_NVCC="ccache nvcc" python3 -m pip install --no-build-isolation .' \
    && env XFORMERS_PT_FLASH_ATTN=0 python3 -m pip install \
       git+https://github.com/huggingface/transformers \
       git+https://github.com/facebookresearch/xformers

# install exllamav2
# sentencepiece
# /root/.cache/torch_extensions/py312_cu124/exllamav2_ext/exllamav2_ext.so
RUN \
    --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    --mount=type=cache,target=/root/.cache/huggingface,sharing=locked \
    git clone https://github.com/turboderp/exllamav2 /opt/exllamav2 \
    && cd /opt/exllamav2 \
    && python3 setup.py build_ext -i \
    && python3 -m pip install --no-deps --no-build-isolation -e .

    # cd /opt/flash-attention \
    # && python3 -m pytest -s -q tests/test_flash_attn.py \
    # && 


RUN \
    --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    --mount=type=cache,target=/root/.cache/huggingface,sharing=locked \
    cd /opt/exllamav2 \
    && if [ ! -d /root/.cache/huggingface/hub/models--turboderp--gpt2-exl2/snapshots/b5411192023974a0c481c71ebda557e3bf9c7ca9 ]; then \
        env HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download turboderp/gpt2-exl2 --revision 3.0bpw; fi \
    && python3 test_inference.py -m /root/.cache/huggingface/hub/models--turboderp--gpt2-exl2/snapshots/b5411192023974a0c481c71ebda557e3bf9c7ca9 -p "A short limmerick: The bear" #|| echo "If you see this, and a complain about no CUDA enabled GPUs, make sure you pass podman build the flag: --device nvidia.com/gpu=all"


#python setup.py build_ext -i \
#    && pip install -e .

RUN \
    --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    git clone https://github.com/theroyallab/tabbyAPI /opt/tabbyAPI \
    && cd /opt/tabbyAPI \
    && python3 -m pip install -e .

# RUN curl -Ls https://github.com/mostlygeek/llama-swap/releases/download/v100/llama-swap_100_linux_amd64.tar.gz \
#     | tar xz -C /usr/local/bin

    # && git checkout 5025c2f1f312703e6810457a20070e5681b264b5 \
RUN \
    --mount=type=cache,target=/root/.cache/go-build,sharing=locked \
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash \
    && bash -c ". /root/.nvm/nvm.sh && nvm install 20 && which npm \
    && curl -Ls https://go.dev/dl/go1.24.4.linux-amd64.tar.gz | tar xz -C /usr/local \
    && cp -s /usr/local/go/bin/* /usr/local/bin \
    && cd /tmp \
    && git clone https://github.com/mostlygeek/llama-swap \
    && cd llama-swap \
    && make clean all \
    && cp /tmp/llama-swap/build/llama-swap-linux-amd64 /usr/local/bin/llama-swap \
    && go install github.com/go-delve/delve/cmd/dlv@latest \
    && ln -s /root/go/bin/dlv /usr/local/bin/dlv"


# ENV PIP_ROOT_USER_ACTION=ignore
# RUN python3 -m pip install flash-attn qwen-vl-utils[decord] torchvision fastapi uvicorn torch pillow gputil 

RUN \
    --mount=type=cache,target=/root/.cache,sharing=locked \
    apt-get update --quiet && apt-get install --assume-yes make curl jq \
    && cd /opt/llama.cpp \
    && git fetch && git pull \
    && cmake --build /opt/llama.cpp/build --parallel --verbose

ENV PATH=/configs:/opt/llama.cpp/build/bin:$PATH

# install exllamav3
RUN \
    --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    --mount=type=cache,target=/root/.cache/huggingface,sharing=locked \
    git clone https://github.com/turboderp-org/exllamav3 /opt/exllamav3 \
    && cd /opt/exllamav3 \
    && python3 setup.py build_ext -i \
    && python3 -m pip install --no-deps --no-build-isolation -e .

RUN curl -Ls https://github.com/mostlygeek/llama-swap/releases/download/v157/llama-swap_157_linux_amd64.tar.gz \
    | tar xz -C /usr/local/bin
