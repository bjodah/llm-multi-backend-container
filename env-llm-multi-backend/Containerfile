#FROM ghcr.io/ggml-org/llama.cpp:server-cuda
FROM docker.io/vllm/vllm-openai:latest


# install llama.cpp
RUN \
    --mount=type=cache,target=/root/.cache,sharing=locked \
    apt-get update --quiet && apt-get install --assume-yes libcurl4-openssl-dev ccache \
    && git clone --recursive --depth 1 --branch master https://github.com/ggml-org/llama.cpp /opt/llama.cpp \
    && cmake \
         -DGGML_CUDA=ON \
         -DGGML_CUDA_FA_ALL_QUANTS=ON \
         -DCMAKE_CUDA_ARCHITECTURES="86" \
         -DLLAMA_CURL=ON \
         -DCMAKE_CUDA_COMPILER_LAUNCHER="ccache" \
         -DCMAKE_C_COMPILER_LAUNCHER="ccache" \
         -DCMAKE_CXX_COMPILER_LAUNCHER="ccache" \
         -S /opt/llama.cpp \
         -B /opt/llama.cpp/build \
    && cmake --build /opt/llama.cpp/build --config Release --parallel

# install exllamav2
RUN \
    git clone https://github.com/turboderp/exllamav2 /opt/exllamav2 \
    && cd /opt/exllamav2 \
    && TORCH_CUDA_ARCH_LIST="8.6" pip install -e .

#python setup.py build_ext -i \
#    && pip install -e .

RUN \
    git clone https://github.com/theroyallab/tabbyAPI /opt/tabbyAPI \
    && cd /opt/tabbyAPI \
    && pip install -e .

# RUN curl -Ls https://github.com/mostlygeek/llama-swap/releases/download/v100/llama-swap_100_linux_amd64.tar.gz \
#     | tar xz -C /usr/local/bin

RUN curl -Ls https://go.dev/dl/go1.24.2.linux-amd64.tar.gz | tar xz -C /usr/local \
    && cp -s /usr/local/go/bin/* /usr/local/bin \
    && cd /tmp \
    && git clone https://github.com/mostlygeek/llama-swap \
    && cd llama-swap \
    && make clean all

RUN apt-get update --quiet && apt-get install --quiet --assume-yes inotify-tools \
    && cp /tmp/llama-swap/build/llama-swap-linux-amd64 /usr/local/bin/llama-swap

COPY watch-and-restart-llama-swap.sh /usr/local/bin/

# ENV PIP_ROOT_USER_ACTION=ignore
# RUN python3 -m pip install flash-attn qwen-vl-utils[decord] torchvision fastapi uvicorn torch pillow gputil 
