# llama.cpp server flags:
#   https://github.com/ggml-org/llama.cpp/tree/master/tools/server
# vLLM serve flags:
#   https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html
# TabbyAPI config:
#   https://github.com/theroyallab/tabbyAPI/wiki/02.-Server-options

version: "3.9"
services:

  llama-swapper:
    name: llm-mb_llama-swapper
    build:
      context: env-llm-multi-backend/
      dockerfile: Containerfile
    #image: bjodah/llm-multi-backend-2025-05-30
    ports:
      - "8686:8686"
    restart: unless-stopped
    devices:
      - "nvidia.com/gpu=all"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ${HOST_CACHE_HUGGINGFACE:-~/.cache/huggingface}:/root/.cache/huggingface
      - ${HOST_CACHE_HUGGINGFACE:-~/.cache/huggingface}/hub:/models  # tabbyAPI...
      - ${HOST_CACHE_LLAMACPP:-~/.cache/llama.cpp}:/root/.cache/llama.cpp
      - ./cache/torch_extensions:/root/.cache/torch_extensions
      - ./logs:/logs
      - ./configs:/configs
      - ./configs/tabby-api_tokens.yml:/opt/tabbyAPI/api_tokens.yml
      - ./configs/tabby-sampler_overrides:/opt/tabbyAPI/sampler_overrides
      - ./qwen2.5-VL-inference-openai:/phildougherty-qwen-vl-api
      #- /opt/gguf-unsloth-maverick:/opt/gguf-unsloth-maverick  # my machine only...
    working_dir: "/"
    security_opt:
      - label:disable
    cap_add:
      - SYS_PTRACE  # gdb debugging
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN
      - HF_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - LLAMA_ARG_THREADS=16
      - LLAMA_ARG_ENDPOINT_METRICS=1
      - LLAMA_ARG_ENDPOINT_SLOTS=1
      - LLAMA_API_KEY=sk-empty
      - LLAMA_ARG_NO_CONTEXT_SHIFT=1
#      - LLAMA_LOG_VERBOSITY=1
      - VLLM_API_KEY=sk-empty
    ipc: host
    entrypoint:
      /configs/autorestart-llama-swap.sh
    command:
      /configs/llama-swap-config.yaml
      /logs/llama-swap.log

  logging-proxy:
    name: llm-mb_logging-proxy
    build:
      context: openai-proxy
      dockerfile: Containerfile
    ports:
      - "8687:8687"
    restart: unless-stopped
    volumes:
      - ./logs:/logs
    environment:
      - OPENAI_PROXY_UNDERLYING=http://llama-swapper:8686
      - OPENAI_PROXY_SQLITE_DB_PATH=/logs/logging-proxy.db
    command: "--host 0.0.0.0 --port 8687"

  chain-of-thought-proxy:
    name: llm-mb_cot-proxy
    build:
      context: cot_proxy
      dockerfile: Containerfile
    ports:
      - "8688:8688"
    restart: unless-stopped
    volumes:
      - ./logs:/logs
      - ./cot_proxy/examples:/configs
    environment:
      - COT_TARGET_BASE_URL=http://logging-proxy:8687/
      - COT_CONFIG=/configs/cot_proxy.yaml
    command: >
      --bind 0.0.0.0:8688
      --workers 4
      --timeout 3000
      --access-logfile /logs/cot-access.log
      --error-logfile /logs/cot-err.log cot_proxy:app

  open-webui-demo:
    name: llm-mb_open-webui
    image: ghcr.io/open-webui/open-webui:main
    environment:
#      - DATA_DIR="/app/backend/data"
      - ENABLE_OLLAMA_API=False
      - ENABLE_OPENAI_API=True
      - OPENAI_API_KEY=sk-empty
      - OPENAI_API_BASE_URL=http://llama-swapper:8686/v1
      - DEFAULT_MODELS="openai/exllamav2-Qwen2.5-Coder-14B-Instruct"
      - WEBUI_AUTH=False
      - DEFAULT_USER_ROLE="admin"
      - HOST=0.0.0.0 # 127.0.0.1
      - PORT=33033
    entrypoint:
      bash
    command:
      start.sh
      #-c "while true; do sleep 1; done"
    # extra_hosts:
    #   - host.docker.internal:host-gateway
    # network_mode: host 
    ports:
      - 33033:33033
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ./data/open-webui:/app/backend/data

  open-webui-pipelines:
    name: llm-mb_open-webui-pipelines
    image: ghcr.io/open-webui/pipelines:main
    extra_hosts:
      - host.docker.internal:host-gateway
    network_mode: host
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ./data/open-webui-pipelines:/app/pipelines:rw
