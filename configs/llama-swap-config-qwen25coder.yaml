
  # the name below works with aider, thanks to:
  # https://github.com/Aider-AI/aider/blob/8f15269bd063a3c720ced514303b9efcc03fe29f/aider/models.py#L418
  llamacpp-Qwen2.5-Coder-32B-Instruct:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-Qwen2.5-Coder-32B-Instruct.log
        --port 8002
        --hf-repo unsloth/Qwen2.5-Coder-32B-Instruct-GGUF:Q4_K_M
        #--hf-repo-draft unsloth/Qwen2.5-Coder-0.5B-Instruct-GGUF:Q4_K_M
        --n-gpu-layers 999 
        #--n-gpu-layers-draft 999
        --ctx-size 32768 
        --cache-type-k q8_0 
        --cache-type-v q5_1 
        --flash-attn
        --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc" 
        --top-k 40 
        --top-p 0.95 
        --min-p 0.01 
        --temp 0.3
        --dry-multiplier 0.5
        --dry-allowed-length 5
    proxy: http://127.0.0.1:8002
    ttl: 3600

  # I use this model for "tab-completion" in emacs (minuet.el), hence small context window.
  llamacpp-Qwen2.5-Coder-7B-Instruct:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-Qwen2.5-Coder-7B-Instruct.log
        --port 8013
        --hf-repo bartowski/Qwen2.5.1-Coder-7B-Instruct-GGUF:Q8_0
        --hf-repo-draft bartowski/Qwen2.5-Coder-0.5B-Instruct-GGUF:Q8_0
        --n-gpu-layers 999
        --n-gpu-layers-draft 999
        --ctx-size 4096 
        --cache-type-k q8_0 
        --cache-type-v q8_0 
        --flash-attn
        --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc" 
        --top-k 40 
        --top-p 0.95
        --min-p 0.0 
        --temp 0.1
        --dry-multiplier 0.1
        --dry-allowed-length 3
    proxy: http://127.0.0.1:8013
    ttl: 3600

  exllamav2-Qwen2.5-Coder-14B-Instruct:
    cmd: >
      python3 /opt/tabbyAPI/main.py
        --config /configs/tabby-config-qwen25-coder-14b.yml
    proxy: http://127.0.0.1:8008
    ttl: 3600
