# I have an workstation (4gb Nvidia T400, intel xeon w-2245, 32gb ram)
# the goal here is to add a few configs that works with that setup.
# I've only bothered building llama.cpp on that mahcine, and I built for compute capability 7.0 (turing)
# Instead of passing --threads flag to each llama.cpp instance: set e.g. LLAMA_ARG_THREADS=8
healthCheckTimeout: 900  # 15minutes, downloading models can take a while

logLevel: debug # info warn

models:

  llamacpp-gemma-3-4b: # 33 pp, 10 tg
    macros:
      context_window: 16384
    cmd: |
      /bld/llama.cpp/bin/llama-server
        --log-file /tmp/llamacpp-gemma-3-4b-it.log
        --port ${PORT}
        --hf-repo unsloth/gemma-3-4b-it-GGUF:Q4_K_XL
        # 2.54 GB
        --n-gpu-layers 99
        --no-mmap
        # of 26   \_ gpu:   pp, tg
        #         /  cpu:   pp, tg
        --ctx-size ${context_window}
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn on
        --temp 1.0
        --min-p 0.01
        --top-k 64
        --top-p 0.95
    proxy: http://127.0.0.1:${PORT}
    metadata:
      context_window: ${context_window}
    ttl: 3600

  llamacpp-Qwen3-0.6B: # 23 pp, 8.5 tg
    macros:
      context_window: 24000
    cmd: |
      /bld/llama.cpp/bin/llama-server
        --log-file /tmp/llamacpp-Qwen3-0.6B.log
        --port ${PORT}
        --hf-repo unsloth/Qwen3-0.6B-GGUF:Q4_K_XL
        # 0.6 GB
        --n-gpu-layers 99
        --jinja
        --cache-type-k q8_0
        --cache-type-v q5_1
        --flash-attn on
        --ctx-size ${context_window}
        --samplers "top_k;dry;min_p;temperature;top_p"
        --min-p 0.005
        --top-p 0.97
        --top-k 40
        --temp 0.7
        --dry-multiplier 0.7
        --dry-allowed-length 4
        --dry-penalty-last-n 2048
        --presence-penalty 0.05
        --frequency-penalty 0.005
        --repeat-penalty 1.01
        --repeat-last-n 16
    proxy: http://127.0.0.1:${PORT}
    metadata:
      context_window: ${context_window}
    ttl: 3600

  llamacpp-Qwen3-1.7B: # 20.2pp, 6.8tg
    macros:
      context_window: 8192
    cmd: |
      /bld/llama.cpp/bin/llama-server
        --log-file /tmp/llamacpp-Qwen3-1.7B.log
        --port ${PORT}
        --hf-repo unsloth/Qwen3-1.7B-GGUF:Q4_K_XL  # 1.13 GB
        --n-gpu-layers 99
        --jinja
        --cache-type-k q8_0
        --cache-type-v q5_1
        --flash-attn on
        --ctx-size ${context_window}
        --samplers "top_k;dry;min_p;temperature;top_p"
        --min-p 0.005
        --top-p 0.97
        --top-k 40
        --temp 0.7
        --dry-multiplier 0.7
        --dry-allowed-length 4
        --dry-penalty-last-n 2048
        --presence-penalty 0.05
        --frequency-penalty 0.005
        --repeat-penalty 1.01
        --repeat-last-n 16
    proxy: http://127.0.0.1:${PORT}
    metadata:
      context_window: ${context_window}
    ttl: 3600

  llamacpp-Qwen3-4B: # yoga720-15ikb: 500tps pp, 15tps tg
    macros:
      context_window: 16384
    cmd: |
      /bld/llama.cpp/bin/llama-server
        --log-file /tmp/llamacpp-Qwen3-4B.log
        --port ${PORT}
        --hf-repo unsloth/Qwen3-4B-GGUF:Q4_K_XL
        # 2.55 GB
        --n-gpu-layers 99
        --jinja
        --cache-type-k q8_0
        --cache-type-v q5_1
        --flash-attn on
        --ctx-size ${context_window}
        --samplers "top_k;dry;min_p;temperature;top_p"
        --min-p 0.005
        --top-p 0.97
        --top-k 40
        --temp 0.7
        --dry-multiplier 0.7
        --dry-allowed-length 4
        --dry-penalty-last-n 2048
        --presence-penalty 0.05
        --frequency-penalty 0.005
        --repeat-penalty 1.01
        --repeat-last-n 16
    proxy: http://127.0.0.1:${PORT}
    metadata:
      context_window: ${context_window}
    ttl: 3600

  llamacpp-Qwen3-30B-A3B:  # yoga720-15ikb: 16tps pp, 7tps tg
    macros:
      context_window: 8192
    cmd: |
      /bld/llama.cpp/bin/llama-server
        --log-file /tmp/llamacpp-Qwen3-30B-A3B.log
        --port ${PORT}
        --hf-repo unsloth/Qwen3-30B-A3B-GGUF:Q4_K_XL
        --n-gpu-layers 99
        --override-tensor '.ffn_.*_exps.=CPU'
        # \_ with gpu + cpu:      pp,      tg
        # /  only       cpu:      pp,      tg
        #--no-mmap
        --jinja
        --cache-type-k q5_1
        --cache-type-v q5_1
        --flash-attn on
        --ctx-size ${context_window}
        --samplers "top_k;dry;min_p;temperature;top_p"
        # ^---- vain attempt to capture both thinking/non-thinking
        --min-p 0.005
        --top-p 0.97
        --top-k 40
        --temp 0.7
        --dry-multiplier 0.7
        --dry-allowed-length 4
        --dry-penalty-last-n 2048
        --presence-penalty 0.05
        --frequency-penalty 0.005
        --repeat-penalty 1.01
        --repeat-last-n 16
    proxy: http://127.0.0.1:${PORT}
    metadata:
      context_window: ${context_window}
    ttl: 3600

  llamacpp-Qwen3-30B-A3B-draft:
    macros:
      context_window: 4096
    cmd: |
      /bld/llama.cpp/bin/llama-server
        --log-file /tmp/llamacpp-Qwen3-30B-A3B.log
        --port ${PORT}
        --hf-repo unsloth/Qwen3-30B-A3B-GGUF:Q4_K_XL
        --hf-repo-draft unsloth/Qwen3-0.6B-GGUF:Q4_K_XL
        #Q8_0
        --n-gpu-layers-draft 99
        --no-mmap
        --jinja
        --ctx-size ${context_window}
        # ^-- I wonder if llama_kv_cache_unified reporting f16 for k and v is the culprit, yup:
        --cache-type-k q5_1
        # ^-- https://github.com/ggml-org/llama.cpp/issues/11200#issuecomment-2585822315
        --cache-type-v q4_0
        --flash-attn on
        --samplers "top_k;dry;min_p;temperature;top_p"
        # vain attempt to capture both thinking/non-thinking
        --min-p 0.005
        --top-p 0.97
        --top-k 40
        --temp 0.7
        --dry-multiplier 0.7
        --dry-allowed-length 4
        --dry-penalty-last-n 2048
        --presence-penalty 0.05
        --frequency-penalty 0.005
        --repeat-penalty 1.01
        --repeat-last-n 16
    proxy: http://127.0.0.1:${PORT}
    metadata:
      context_window: ${context_window}
    ttl: 3600


  llamacpp-Qwen2.5-Coder-3B:
    # Define a macro for the context window so it can be reused.
    macros:
      context_window: 8192
    cmd: |
      /bld/llama.cpp/bin/llama-server
        --log-file /tmp/llamacpp-Qwen2.5-Coder-3B-Instruct.log
        --port ${PORT}
        --hf-repo unsloth/Qwen2.5-Coder-3B-Instruct-GGUF:Q5_K_M
        # 2.22GB
        --n-gpu-layers 99
        --ctx-size ${context_window}
        --cache-type-k q8_0 
        --cache-type-v q8_0 
        --flash-attn on
        --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc" 
        --top-k 40 
        --top-p 0.95
        --min-p 0.0 
        --temp 0.1
        --dry-multiplier 0.1
        --dry-allowed-length 3
    metadata:
      context_window: ${context_window}
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600

  llamacpp-Phi-4-mini:
    # Define a macro for the context window so it can be reused.
    macros:
      context_window: 8192
    cmd: |
      /bld/llama.cpp/bin/llama-server
        --log-file /tmp/llamacpp-Phi-4.log
        --port ${PORT}
        --hf-repo bartowski/microsoft_Phi-4-mini-instruct-GGUF:Q4_K_L
        # 2.64
        --n-gpu-layers 24
        # of 32, (k4,q4,4096): 1.894GB vram
        --ctx-size ${context_window}
        --cache-type-k q8_0
        --cache-type-v q5_1
        --flash-attn on
        --samplers "min_p;dry;temperature"
        # xtc;
        --min-p 0.03
        --dry-multiplier 1.0
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 0.5
    metadata:
      context_window: ${context_window}
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600


  llamacpp-Ling-Coder-lite:
    # Define a macro for the context window so it can be reused.
    macros:
      context_window: 16384
    cmd: |
      /bld/llama.cpp/bin/llama-server
        --log-file /tmp/llamacpp-Ling-Coder-lite.log
        --port ${PORT}
        --hf-repo mradermacher/Ling-Coder-lite-i1-GGUF:Q4_K_M
        # 11.2GB
        --n-gpu-layers 99                       
        --override-tensor '.ffn_.*_exps.=CPU'
        # \_ with gpu + cpu:      pp,      tg
        # /  only       cpu:      pp,      tg
        --no-mmap
        --ctx-size ${context_window}
        --cache-type-k q8_0 
        --cache-type-v q5_1
        --flash-attn on
        --samplers "top_k;dry;top_p;min_p;temperature;typ_p;xtc" 
        --top-k 40 
        --dry-multiplier 0.5 
        --dry-allowed-length 5
        --top-p 0.95 
        --min-p 0.01 
        --temp 0.8 
    metadata:
      context_window: ${context_window}
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600

  llamacpp-ling-lite-0415:
    # Define a macro for the context window so it can be reused.
    macros:
      context_window: 16384
    cmd: |
      /bld/llama.cpp/bin/llama-server
        --log-file /tmp/llamacpp-ling-lite-0415.log
        --port ${PORT}
        --hf-repo bartowski/inclusionAI_Ling-lite-0415-GGUF:Q4_K_L
        # 11.4GB
        --n-gpu-layers 28                       
        --override-tensor '.ffn_.*_exps.=CPU'
        # of 28,  \_ with gpu + cpu:      pp,      tg
        #         /  only       cpu:      pp,      tg
        --no-mmap
        --ctx-size ${context_window}
        --cache-type-k q8_0 
        --cache-type-v q5_1
        --flash-attn on
        --samplers "min_p;dry;xtc;temperature"
        --min-p 0.01
        --dry-multiplier 1.1
        # 1.0
        --dry-allowed-length 4
        # 3
        --dry-penalty-last-n 1024
        --temp 0.6
        #--verbose
    metadata:
      context_window: ${context_window}
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600

