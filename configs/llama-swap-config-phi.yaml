  llamacpp-Phi-4:
    # Define a macro for the context window so it can be reused.
    macros:
      context_window: 32768
    cmd: |
      llama-server --log-file   /logs/llamacpp-Phi-4.log
        --port ${PORT}
        --hf-repo unsloth/phi-4-GGUF:Q6_K
        # Q2_K_L, bartowski/phi-4-GGUF:Q8_0
        --n-gpu-layers 999
        --jinja
        --ctx-size ${context_window}
        --cache-type-k q8_0
        # --cache-type-v q8_0
        # --flash-attn
        --samplers 'min_p;dry;temperature'
        # xtc;
        --min-p 0.03
        --dry-multiplier 1.0
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 0.5
    metadata:
      context_window: ${context_window}
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600

  llamacpp-Phi-4-reasoning-plus:
    # Define a macro for the context window so it can be reused.
    macros:
      context_window: 32768
    cmd: |
      llama-server --log-file   /logs/llamacpp-Phi-4-reasoning-plus.log
        --port ${PORT}
        --hf-repo unsloth/Phi-4-reasoning-plus-GGUF:Q6_K_XL
        --n-gpu-layers 99
        --jinja
        --special
        # ^--- response will contain 
        --ctx-size ${context_window}
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn
        --samplers 'top_k;dry;top_p;min_p;temperature;typ_p;xtc'
        --top-k 50
        --dry-multiplier 1.2
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --top-p 0.95
        --min-p 0.01
        --temp 0.8
        --repeat-penalty 1.05
        --frequency-penalty 0.01
        --repeat-last-n 16
        #--verbose

    metadata:
      context_window: ${context_window}
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600
