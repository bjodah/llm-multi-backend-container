  llamacpp-Phi-4:
    cmd: |
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-Phi-4.log
        --port 8006
        --hf-repo unsloth/phi-4-GGUF:Q6_K
        # Q2_K_L, bartowski/phi-4-GGUF:Q8_0
        --n-gpu-layers 999
        --ctx-size 32768
        --cache-type-k q8_0
        # --cache-type-v q8_0
        # --flash-attn
        --samplers 'min_p;dry;temperature'
        # xtc;
        --min-p 0.03
        --dry-multiplier 1.0
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 0.5
    proxy: http://127.0.0.1:8006
    ttl: 3600

  llamacpp-Phi-4-reasoning-plus:
    cmd: |
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-Phi-4-reasoning-plus.log
        --port 8633
        --hf-repo unsloth/Phi-4-reasoning-plus-GGUF:Q6_K_XL
        --n-gpu-layers 99
        --jinja
        --special
        # ^--- response will contain <think> </think> <|im_end|>
        --ctx-size 32768
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn
        --samplers 'top_k;dry;top_p;min_p;temperature;typ_p;xtc'
        --top-k 50
        --dry-multiplier 1.2
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --top-p 0.95
        --min-p 0.01
        --temp 0.8
        --repeat-penalty 1.05
        --frequency-penalty 0.01
        --repeat-last-n 16
        #--verbose

    proxy: http://127.0.0.1:8633
    ttl: 3600
