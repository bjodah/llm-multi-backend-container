  llamacpp-gemma-3-12b:
    cmd: |
      llama-server --log-file   /logs/llamacpp-gemma-3-12b.log
        --port ${PORT}
        --hf-repo unsloth/gemma-3-12b-it-GGUF:Q6_K_XL
        --n-gpu-layers 999
        # --hf-repo-draft google/gemma-3-1b-it-qat-q4_0-gguf
        # --n-gpu-layers-draft 999
        --jinja
        --ctx-size 28000
        #--cache-type-k q8_0
        # --cache-type-v q8_0  https://github.com/ggml-org/llama.cpp/issues/12352#issuecomment-2727452955
        --flash-attn on
        # xtc;
        --samplers 'min_p;dry;temperature;top_p'
        --min-p 0.03
        --dry-multiplier 0.2
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 1.0
        --top-p 0.95
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600

  # https://www.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/
  # TODO: add --mmproj flag for enabling multimodal input?
  llamacpp-gemma-3-27b:
    cmd: |
      llama-server --log-file   /logs/llamacpp-gemma-3-27b.log
        --port ${PORT}
        --hf-repo google/gemma-3-27b-it-qat-q4_0-gguf
        --n-gpu-layers 999
        # --hf-repo-draft google/gemma-3-1b-it-qat-q4_0-gguf
        # --n-gpu-layers-draft 999
        --jinja
        --ctx-size 49000
        # --cache-type-k q8_0
        # --cache-type-v q5_1
        --no-mmproj
        --flash-attn on
        --samplers 'min_p;dry;temperature;top_p'
        # xtc;
        --min-p 0.03
        --dry-multiplier 0.2
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 1.0
        --top-p 0.95
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600

  llamacpp-gemma-3-4b:
    cmd: |
      llama-server --log-file   /logs/llamacpp-gemma-3-4b.log
        --port ${PORT}
        #--hf-repo google/gemma-3-4b-it-qat-q4_0-gguf
        --hf-repo unsloth/gemma-3-4b-it-GGUF:Q8_0
        # --cache-type-k q8_0
        # --cache-type-v q8_0
        --flash-attn on
        --n-gpu-layers 999
        --jinja
        --ctx-size 32000
        --temp 1.0
        --prio 2
        --temp 1.0
        --repeat-penalty 1.0
        --min-p 0.01
        --top-k 64
        --top-p 0.95
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600

  llamacpp-gemma-3-1b:
    cmd: |
      llama-server --log-file   /logs/llamacpp-gemma-3-1b.log
        --port ${PORT}
        --hf-repo unsloth/gemma-3-1b-it-GGUF:Q8_0
        --n-gpu-layers 999
        --jinja
        --ctx-size 32768
        # --cache-type-k q8_0
        # --cache-type-v q8_0
        --flash-attn on
        --temp 1.0
        --min-p 0.01
        --top-k 64
        --top-p 0.95
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600

  llamacpp-gemma-3-270m-it-qat:
    cmd: |
      llama-server --log-file   /logs/llamacpp-llama3-270m.log
        --port ${PORT}
        #--hf-repo bartowski/google_gemma-3-270m-it-qat-GGUF:Q4_0
        #--hf-repo unsloth/gemma-3-270m-it-qat-GGUF:Q4_K_XL
        --hf-repo unsloth/gemma-3-270m-it-qat-GGUF:Q8_K_XL
        --n-gpu-layers 128
        --jinja
        --ctx-size 32768
        --cache-type-k q8_0
        --cache-type-v q5_1
        --flash-attn on
        --temp 1.0
        --min-p 0.01
        --top-k 64
        --top-p 0.95

  llamacpp-gemma-3-270m-it:
    cmd: |
      llama-server --log-file   /logs/llamacpp-llama3-270m.log
        --port ${PORT}
        #--hf-repo bartowski/google_gemma-3-270m-it-GGUF:Q8_0
        #--hf-repo unsloth/gemma-3-270m-it-GGUF:Q4_K_XL
        --hf-repo unsloth/gemma-3-270m-it-GGUF:Q8_K_XL
        --n-gpu-layers 128
        --jinja
        --ctx-size 32768
        --cache-type-k q8_0
        --cache-type-v q5_1
        --flash-attn on
        --temp 1.0
        --min-p 0.01
        --top-k 64
        --top-p 0.95
