# llama-swap config. Note that there are environment variables set in compose.yml that
# affects the behviour of some inference engines, grep for e.g. LLAMA_ARG_THREADS, VLLM_API_KEY, etc.
healthCheckTimeout: 900  # 15minutes, downloading models can take a while

logLevel: debug # info warn

# - meta-commentary
#   - llama.cpp)
#         - I'm now avoiding `--repeat-penalty`, preferring `--dry-multiplifer`
#         `--prio` flag (need to figure out what does values do, responsible for spinning 1 thread 100%?)
#         - No longer setting `--seed "-1"`, default and respawning llama-swap for setting deterministic is not practical.


models:

  # Mistral-3.1-2503 has vision support, for which text generation suffers?
  # Neither is is it particularly good at coding either (Qwen2.5-Coder is better)
  llamacpp-mistral-small-24b-2501:
    cmd: |
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-mistral-small-24b-2501.log
        --port 8005
        --hf-repo unsloth/Mistral-Small-24B-Instruct-2501-GGUF:Q6_K
        --n-gpu-layers 99
        --jinja
        # --hf-repo-draft bartowski/alamios_Mistral-Small-3.1-DRAFT-0.5B-GGUF:Q8_0
        # --n-gpu-layers-draft 99
        --ctx-size 24000
        --cache-type-k q8_0
        # --cache-type-v q8_0
        # --flash-attn
        --samplers 'min_p;dry;xtc;temperature'
        --min-p 0.03
        --dry-multiplier 1.0
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 0.15

    proxy: http://127.0.0.1:8005
    ttl: 3600

  # the name below works with aider, thanks to:
  # https://github.com/Aider-AI/aider/blob/8f15269bd063a3c720ced514303b9efcc03fe29f/aider/models.py#L429
  llamacpp-QwQ-32B:
    cmd: |
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-QwQ-32B.log
        --port 8001
        --hf-repo unsloth/QwQ-32B-GGUF:Q4_K_XL
        --n-gpu-layers 99
        --jinja
        --ctx-size 32768
        --cache-type-k q8_0
        --cache-type-v q5_1
        --flash-attn
        --samplers 'top_k;dry;top_p;min_p;temperature;typ_p;xtc'
        --top-k 50
        --dry-multiplier 0.25
        --dry-allowed-length 5
        --top-p 0.92
        --min-p 0.01
        --temp 0.5
        # was 0.6
        #--verbose

    proxy: http://127.0.0.1:8001
    ttl: 3600
        
  llamacpp-DeepCoder-14B:
    cmd: |
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-DeepCoder-14B.log
        --port 8017
        --hf-repo bartowski/agentica-org_DeepCoder-14B-Preview-GGUF:Q4_K_L
        # :Q8_0
        --n-gpu-layers 999
        --jinja
        --ctx-size 65536
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn
        #--samplers "min_p;dry;temperature"  # xtc;
        --temp 0.6
        --top-p 0.95
        #--min-p 0.01
        # --dry-multiplier 1.0
        # --dry-allowed-length 3
        # --dry-penalty-last-n 256
    proxy: http://127.0.0.1:8017
    ttl: 3600

  exllamav2-gemma-3-27b:
    cmd: |
      env --chdir /opt/tabbyAPI python3 main.py
        --config /configs/tabby-config-gemma-3-27b.yml
    proxy: http://127.0.0.1:7567
    ttl: 3600

  exllamav2-QwQ-32B:
    cmd: |
      env --chdir /opt/tabbyAPI python3 main.py
        --config /configs/tabby-config-qwq32b.yml
    proxy: http://127.0.0.1:8007
    ttl: 3600


  vllm-SmolLM2-1.7B-Instruct:
    cmd: |
      python3 -m vllm.entrypoints.openai.api_server
        --port 8009
        --served-model-name vllm-SmolLM2-1.7B-Instruct
        --model HuggingFaceTB/SmolLM2-1.7B-Instruct
        --gpu-memory-utilization 0.5
        --max-model-len 2048
    proxy: http://127.0.0.1:8009
    ttl: 3600

  vllm-Qwen-QwQ-32B:
    cmd: |
      env VLLM_ATTENTION_BACKEND=FLASHINFER python3
        -m vllm.entrypoints.openai.api_server
        --port 8010
        --served-model-name vllm-Qwen-QwQ-32B
        --model Qwen/QwQ-32B-AWQ
        --trust-remote-code
        --gpu-memory-utilization 0.97
        --enable-chunked-prefill
        --enable-prefix-caching
        --max-model-len 16500
        --max-num-seqs 128
        --max-num-batched-tokens 32768
        --disable-sliding-window
        --generation-config Qwen/QwQ-32B-AWQ
        --enable-reasoning
        --reasoning-parser deepseek_r1
        --tool-call-parser hermes 
        --kv-cache-dtype fp8_e5m2
    proxy: http://127.0.0.1:8010
    ttl: 3600
    # 
    # --max-model-len 32768
    #  --kv-cache-dtype fp8_e4m3
    # --enable-auto-tool-choice

  vllm-Qwen3-14B:  # ~30 tps for 3090@250W
    cmd: |
      env VLLM_ATTENTION_BACKEND=FLASHINFER python3
        -m vllm.entrypoints.openai.api_server
        --port 8015
        --served-model-name vllm-Qwen3-14B
        --model Qwen/Qwen3-14B-AWQ
        --trust-remote-code
        --gpu-memory-utilization 0.97
        --enable-chunked-prefill
        --enable-prefix-caching
        --max-model-len 32768
        --max-num-seqs 128
        --max-num-batched-tokens 32768
        --disable-sliding-window
        --generation-config Qwen/Qwen3-14B-AWQ
        --enable-reasoning
        --reasoning-parser deepseek_r1
        --kv-cache-dtype fp8_e5m2
    proxy: http://127.0.0.1:8015
    ttl: 3600

  vllm-Qwen2.5-VL-7B:
    cmd: |
      python3 -m vllm.entrypoints.openai.api_server
          --api-key sk-empty
          --port 8014
          --served-model-name vllm-Qwen2.5-VL-7B
          --model Qwen/Qwen2.5-VL-7B-Instruct-AWQ
          --trust-remote-code
          --gpu-memory-utilization 0.954330
          --max-model-len 8192
          --max-num-batched-tokens 32768
    proxy: http://127.0.0.1:8014
    ttl: 3600
    # --kv-cache-dtype fp8_e5m2
    # (--max-model-len) 32768
    # --enable-chunked-prefill


 
  # I'm not yet sure what Maverick is good at. Hopefully it could be useful as an offline knowledge base
  # for any other tasks, there are better models (for local inference).
  llamacpp-llama4-maverick:
    cmd: |
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-llama4-maverick.log
        --port 8016
        --model /root/.cache/huggingface/hub/models--unsloth--Llama-4-Maverick-17B-128E-Instruct-GGUF/snapshots/d68803567c756664117d0de7da050bf0ec1bb092/UD-Q2_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q2_K_XL-00001-of-00004.gguf
        --n-gpu-layers 99
        --override-tensor '([0-9]+).ffn_.*_exps.=CPU'
        --ubatch-size 1
        --jinja
        --override-kv llama4.expert_used_count=int:1
        --ctx-size 16384
        --samplers 'min_p;dry;xtc;temperature'
        --min-p 0.03
        --dry-multiplier 1.0
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 0.6
    # --override-kv llama4.expert_used_count=int:2
    proxy: http://127.0.0.1:8016
    ttl: 3600
  
  llamacpp-openthinker2-32b:
    cmd: |
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-openthinker2-32b.log
        --port 8024
        --hf-repo bartowski/open-thoughts_OpenThinker2-32B-GGUF
        --n-gpu-layers 999
        --jinja
        --ctx-size 16000
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn
        --samplers 'min_p;dry;xtc;temperature'
        --min-p 0.01
        --dry-multiplier 1.1
        # 1.0
        --dry-allowed-length 4
        # 3
        --dry-penalty-last-n 1024
        --temp 0.6
    proxy: http://127.0.0.1:8024
    ttl: 3600


  # see https://github.com/LG-AI-EXAONE/EXAONE-Deep
  llamacpp-exaone-deep-32b:
    cmd: |
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-exaone-deep-32b
        --port 8028
        --hf-repo bartowski/LGAI-EXAONE_EXAONE-Deep-32B-GGUF:Q4_K_M
        # 19.3GB
        --n-gpu-layers 99
        --jinja
        --ctx-size 32768
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn
        --samplers 'min_p;dry;xtc;temperature'
        --min-p 0.02
        --dry-multiplier 0.5
        --dry-allowed-length 3
        --dry-penalty-last-n 4096
        --temp 0.7
        # 0.6 recommended, with top_p=0.95
    proxy: http://127.0.0.1:8028
    ttl: 3600
