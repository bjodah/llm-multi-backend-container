# llama-swap config. Note that there are environment variables set in compose.yml that
# affects the behviour of some inference engines, grep for e.g. LLAMA_ARG_THREADS, VLLM_API_KEY, etc.
healthCheckTimeout: 900  # 15minutes, downloading models can take a while

logLevel: debug # info warn

# - meta-commentary
#   - llama.cpp)
#         - I'm now avoiding `--repeat-penalty`, preferring `--dry-multiplifer`
#         `--prio` flag (need to figure out what does values do, responsible for spinning 1 thread 100%?)
#         - No longer setting `--seed "-1"`, default and respawning llama-swap for setting deterministic is not practical.


models:

  llamacpp-gemma-3-1b-it:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-gemma-3-1b-it.log
        --port 8012
        --hf-repo unsloth/gemma-3-1b-it-GGUF:Q8_0
        --n-gpu-layers 999
        --ctx-size 16000
        --cache-type-k q4_0
        --cache-type-v q4_0
        --flash-attn
        --temp 1.0
        --min-p 0.01
        --top-k 64
        --top-p 0.95
    proxy: http://127.0.0.1:8012
    ttl: 3600

  # I use this model for "tab-completion" in emacs (minuet.el), hence small context window.
  llamacpp-Qwen2.5-Coder-7B-Instruct:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-Qwen2.5-Coder-7B-Instruct.log
        --port 8013
        --hf-repo bartowski/Qwen2.5.1-Coder-7B-Instruct-GGUF:Q8_0
        --hf-repo-draft bartowski/Qwen2.5-Coder-0.5B-Instruct-GGUF:Q8_0
        --n-gpu-layers 999
        --n-gpu-layers-draft 999
        --ctx-size 4096 
        --cache-type-k q8_0 
        --cache-type-v q8_0 
        --flash-attn
        --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc" 
        --top-k 40 
        --top-p 0.95
        --min-p 0.0 
        --temp 0.1
        --dry-multiplier 0.1
        --dry-allowed-length 3
    proxy: http://127.0.0.1:8013
    ttl: 3600

  # the name below works with aider, thanks to:
  # https://github.com/Aider-AI/aider/blob/8f15269bd063a3c720ced514303b9efcc03fe29f/aider/models.py#L429
  llamacpp-QwQ-32B:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-QwQ-32B.log
        --port 8001
        --hf-repo unsloth/QwQ-32B-GGUF:Q4_K_XL
        --n-gpu-layers 99
        --jinja
        --ctx-size 32768
        --cache-type-k q8_0
        --cache-type-v q5_1
        --flash-attn
        --samplers "top_k;dry;top_p;min_p;temperature;typ_p;xtc"
        --top-k 50
        --dry-multiplier 0.25
        --dry-allowed-length 5
        --top-p 0.92
        --min-p 0.01
        --temp 0.5  # was 0.6
        #--verbose

    proxy: http://127.0.0.1:8001
    ttl: 3600

  # the name below works with aider, thanks to:
  # https://github.com/Aider-AI/aider/blob/8f15269bd063a3c720ced514303b9efcc03fe29f/aider/models.py#L418
  llamacpp-Qwen2.5-Coder-32B-Instruct:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-Qwen2.5-Coder-32B-Instruct.log
        --port 8002
        --hf-repo unsloth/Qwen2.5-Coder-32B-Instruct-GGUF:Q4_K_M
        #--hf-repo-draft unsloth/Qwen2.5-Coder-0.5B-Instruct-GGUF:Q4_K_M
        --n-gpu-layers 999 
        #--n-gpu-layers-draft 999
        --ctx-size 32768 
        --cache-type-k q8_0 
        --cache-type-v q5_1 
        --flash-attn
        --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc" 
        --top-k 40 
        --top-p 0.95 
        --min-p 0.01 
        --temp 0.3
        --dry-multiplier 0.5
        --dry-allowed-length 5
    proxy: http://127.0.0.1:8002
    ttl: 3600

  llamacpp-gemma-3-27b-it:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-gemma-3-27b-it.log
        --port 8003
        --hf-repo google/gemma-3-27b-it-qat-q4_0-gguf
        --n-gpu-layers 999
        # --hf-repo-draft google/gemma-3-1b-it-qat-q4_0-gguf
        # --n-gpu-layers-draft 999
        --ctx-size 21000
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn
        --samplers "min_p;dry;temperature;top_p" # xtc;
        --min-p 0.03
        --dry-multiplier 0.2
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 1.0
        --top-p 0.95
    proxy: http://127.0.0.1:8003
    ttl: 3600

  llamacpp-gemma-3-4b-it:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-gemma-3-4b-it.log
        --port 8004
        --hf-repo google/gemma-3-4b-it-qat-q4_0-gguf # unsloth/gemma-3-4b-it-GGUF:Q4_K_M
        --cache-type-k q4_0
        --cache-type-v q4_0
        --flash-attn
        --n-gpu-layers 999
        --ctx-size 16000
        --temp 1.0
        --prio 2
        --temp 1.0
        --repeat-penalty 1.0
        --min-p 0.01
        --top-k 64
        --top-p 0.95
    proxy: http://127.0.0.1:8004
    ttl: 3600

  # Mistral-3.1-2503 has vision support, for which text generation suffers?
  # Neither is is it particularly good at coding either (Qwen2.5-Coder is better)
  llamacpp-mistral-small-24b-it-2501:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-mistral-small-24b-it-2501.log
        --port 8005
        --hf-repo unsloth/Mistral-Small-24B-Instruct-2501-GGUF:Q6_K
        --n-gpu-layers 99
        # --hf-repo-draft bartowski/alamios_Mistral-Small-3.1-DRAFT-0.5B-GGUF:Q8_0
        # --n-gpu-layers-draft 99
        --ctx-size 24000
        --cache-type-k q8_0
        # --cache-type-v q8_0
        # --flash-attn
        --samplers "min_p;dry;xtc;temperature"
        --min-p 0.03
        --dry-multiplier 1.0
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 0.15

    proxy: http://127.0.0.1:8005
    ttl: 3600

  llamacpp-Phi-4:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-Phi-4.log
        --port 8006
        --hf-repo unsloth/phi-4-GGUF:Q6_K  # Q2_K_L, bartowski/phi-4-GGUF:Q8_0
        --n-gpu-layers 999
        --ctx-size 32768
        --cache-type-k q8_0
        # --cache-type-v q8_0
        # --flash-attn
        --samplers "min_p;dry;temperature"  # xtc;
        --min-p 0.03
        --dry-multiplier 1.0
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 0.5
    proxy: http://127.0.0.1:8006
    ttl: 3600

  llamacpp-Phi-4-reasoning-plus:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-Phi-4-reasoning-plus.log
        --port 8633
        --hf-repo unsloth/Phi-4-reasoning-plus-GGUF:Q6_K_XL
        --n-gpu-layers 99
        --jinja
        --special  # response will contain <think> </think> <|im_end|>
        --ctx-size 32768
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn
        --samplers "top_k;dry;top_p;min_p;temperature;typ_p;xtc"
        --top-k 50
        --dry-multiplier 1.2
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --top-p 0.95
        --min-p 0.01
        --temp 0.8
        --repeat-penalty 1.05
        --frequency-penalty 0.01
        --repeat-last-n 16
        #--verbose

    proxy: http://127.0.0.1:8633
    ttl: 3600
        
  llamacpp-DeepCoder-14B:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-DeepCoder-14B.log
        --port 8017
        --hf-repo bartowski/agentica-org_DeepCoder-14B-Preview-GGUF:Q4_K_L  # :Q8_0
        --n-gpu-layers 999
        --ctx-size 65536
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn
        #--samplers "min_p;dry;temperature"  # xtc;
        --temp 0.6
        --top-p 0.95
        #--min-p 0.01
        # --dry-multiplier 1.0
        # --dry-allowed-length 3
        # --dry-penalty-last-n 256
    proxy: http://127.0.0.1:8017
    ttl: 3600

  exllamav2-QwQ-32B:
    cmd: >
      python3 /opt/tabbyAPI/main.py
        --config /configs/tabby-config-qwq32b.yml
    proxy: http://127.0.0.1:8007
    ttl: 3600

  exllamav2-Qwen2.5-Coder-14B-Instruct:
    cmd: >
      python3 /opt/tabbyAPI/main.py
        --config /configs/tabby-config-qwen25-coder-14b.yml
    proxy: http://127.0.0.1:8008
    ttl: 3600

  vllm-SmolLM2-1.7B-Instruct:
    cmd: >
      python3 -m vllm.entrypoints.openai.api_server
        --port 8009
        --served-model-name vllm-SmolLM2-1.7B-Instruct
        --model HuggingFaceTB/SmolLM2-1.7B-Instruct
        --gpu-memory-utilization 0.5
        --max-model-len 2048
    proxy: http://127.0.0.1:8009
    ttl: 3600

  vllm-Qwen-QwQ-32B:
    cmd: >
      env TORCH_CUDA_ARCH_LIST=8.6 VLLM_ATTENTION_BACKEND=FLASHINFER python3
        -m vllm.entrypoints.openai.api_server
        --port 8010
        --served-model-name vllm-Qwen-QwQ-32B
        --model Qwen/QwQ-32B-AWQ
        --trust-remote-code
        --gpu-memory-utilization 0.97
        --enable-chunked-prefill
        --enable-prefix-caching
        --max-model-len 16500
        --max-num-seqs 128
        --max-num-batched-tokens 32768
        --disable-sliding-window
        --generation-config Qwen/QwQ-32B-AWQ
        --enable-reasoning
        --reasoning-parser deepseek_r1
        --tool-call-parser hermes 
        --kv-cache-dtype fp8_e5m2
    proxy: http://127.0.0.1:8010
    ttl: 3600
    # 
    # --max-model-len 32768
    #  --kv-cache-dtype fp8_e4m3
    # --enable-auto-tool-choice

  vllm-Qwen3-14B:  # ~30 tps for 3090@250W
    cmd: >
      env TORCH_CUDA_ARCH_LIST=8.6 VLLM_ATTENTION_BACKEND=FLASHINFER python3
        -m vllm.entrypoints.openai.api_server
        --port 8015
        --served-model-name vllm-Qwen3-14B
        --model Qwen/Qwen3-14B-AWQ
        --trust-remote-code
        --gpu-memory-utilization 0.97
        --enable-chunked-prefill
        --enable-prefix-caching
        --max-model-len 32768
        --max-num-seqs 128
        --max-num-batched-tokens 32768
        --disable-sliding-window
        --generation-config Qwen/Qwen3-14B-AWQ
        --enable-reasoning
        --reasoning-parser deepseek_r1
        --kv-cache-dtype fp8_e5m2
    proxy: http://127.0.0.1:8015
    ttl: 3600

  vllm-Qwen2.5-VL-7B:
    cmd: >
      python3 -m vllm.entrypoints.openai.api_server
          --api-key sk-empty
          --port 8014
          --served-model-name vllm-Qwen2.5-VL-7B
          --model Qwen/Qwen2.5-VL-7B-Instruct-AWQ
          --trust-remote-code
          --gpu-memory-utilization 0.954330
          --max-model-len 8192
          --max-num-batched-tokens 32768
    proxy: http://127.0.0.1:8014
    ttl: 3600
    # --kv-cache-dtype fp8_e5m2
    # (--max-model-len) 32768
    # --enable-chunked-prefill


  llamacpp-Ling-Coder-lite:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-Ling-Coder-lite.log
        --port 8011
        --hf-repo mradermacher/Ling-Coder-lite-GGUF:Q4_K_M
        --n-gpu-layers 99 
        --ctx-size 32768
        --cache-type-k q8_0 
        --cache-type-v q5_1
        --flash-attn
        --samplers "top_k;dry;top_p;min_p;temperature;typ_p;xtc" 
        --top-k 40 
        --dry-multiplier 0.5 
        --dry-allowed-length 5
        --top-p 0.95 
        --min-p 0.01 
        --temp 0.8 

    proxy: http://127.0.0.1:8011
    ttl: 3600
 
  # I'm not yet sure what Maverick is good at. Hopefully it could be useful as an offline knowledge base
  # for any other tasks, there are better models (for local inference).
  llamacpp-llama4-maverick:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-llama4-maverick.log
        --port 8016
        --model /root/.cache/huggingface/hub/models--unsloth--Llama-4-Maverick-17B-128E-Instruct-GGUF/snapshots/d68803567c756664117d0de7da050bf0ec1bb092/UD-Q2_K_XL/Llama-4-Maverick-17B-128E-Instruct-UD-Q2_K_XL-00001-of-00004.gguf
        --n-gpu-layers 99
        --override-tensor '([0-9]+).ffn_.*_exps.=CPU'
        --ubatch-size 1
        --jinja
        --override-kv llama4.expert_used_count=int:1
        --ctx-size 16384
        --samplers "min_p;dry;xtc;temperature"
        --min-p 0.03
        --dry-multiplier 1.0
        --dry-allowed-length 3
        --dry-penalty-last-n 256
        --temp 0.6
    # --override-kv llama4.expert_used_count=int:2
    proxy: http://127.0.0.1:8016
    ttl: 3600

  llamacpp-glm-4-32b-0414:  # ~16 tps TG
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-glm-4-32b-0414.log
        --port 8018
        --hf-repo bartowski/THUDM_GLM-4-32B-0414-GGUF:Q4_K_M # Q5_K_M
        #--hf-repo unsloth/GLM-4-32B-0414-GGUF:Q4_K_XL
        --n-gpu-layers 999
        --override-kv tokenizer.ggml.eos_token_id=int:151336
        --override-kv glm4.rope.dimension_count=int:64
        --chat-template chatglm4
        --ctx-size 31000  # 32768
        --cache-type-k q8_0 
        # --cache-type-v q8_0  # TODO: try with q8_0, maybe lower context?
        # --flash-attn
        --samplers "min_p;dry;temperature;xtc"
        --dry-multiplier 1.2 # 1.0 1.1
        --dry-allowed-length 4  # 3
        --dry-penalty-last-n 2048
        --min-p 0.02
        --temp 0.6
    proxy: http://127.0.0.1:8018
    ttl: 3600

  # Thinking version of glm-4
  llamacpp-glm-z1-32b-0414:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-glm-z1-32b-0414.log
        --port 8019
        --hf-repo bartowski/THUDM_GLM-Z1-32B-0414-GGUF:Q3_K_XL # IQ4_XS # Q4_K_L, Q5_K_S
        --n-gpu-layers 999
        --override-kv tokenizer.ggml.eos_token_id=int:151336
        --override-kv glm4.rope.dimension_count=int:64
        --chat-template chatglm4
        --ctx-size 22000  # 32768
        --cache-type-k q8_0 
        # --cache-type-v q8_0
        # --flash-attn
        --samplers "min_p;dry;xtc;temperature"
        --min-p 0.01
        --dry-multiplier 0.5
        --dry-allowed-length 3
        --temp 0.4 # 0.6
    proxy: http://127.0.0.1:8019
    ttl: 3600

  # I haven't had too much success with this, I prefer non-rumination
  llamacpp-glm-z1-rumination-32b-0414:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-glm-z1-rumination-32b-0414.log
        --port 8020
        --hf-repo bartowski/THUDM_GLM-Z1-Rumination-32B-0414-GGUF:Q3_K_XL #IQ4_XS  # Q4_K_L, Q5_K_S
        --n-gpu-layers 999
        --override-kv tokenizer.ggml.eos_token_id=int:151336
        --override-kv glm4.rope.dimension_count=int:64
        --chat-template chatglm4
        --ctx-size 22000 # can be pushed to ~24000
        --cache-type-k q8_0 
        # --cache-type-v q8_0 
        # --flash-attn
        --samplers "min_p;dry;xtc;temperature"
        --min-p 0.01
        --dry-multiplier 1.1 # 1.0
        --dry-allowed-length 4  # 3
        --dry-penalty-last-n 1024
        --temp 0.4  # 0.6
    proxy: http://127.0.0.1:8020
    ttl: 3600
  
  llamacpp-ling-lite-0415:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-ling-lite-0415.log
        --port 8022
        --hf-repo bartowski/inclusionAI_Ling-lite-0415-GGUF:Q6_K_L
        --ctx-size 32768
        --n-gpu-layers 99 
        --cache-type-k q8_0 
        #--cache-type-v q8_0
        #--flash-attn  # <-- gibberish for large context?
        --samplers "min_p;dry;xtc;temperature"
        --min-p 0.01
        --dry-multiplier 1.1 # 1.0
        --dry-allowed-length 4  # 3
        --dry-penalty-last-n 1024
        --temp 0.6
        #--verbose
    proxy: http://127.0.0.1:8022
    ttl: 3600

  llamacpp-openthinker2-32b:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-openthinker2-32b.log
        --port 8024
        --hf-repo bartowski/open-thoughts_OpenThinker2-32B-GGUF
        --n-gpu-layers 999
        --ctx-size 16000
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn
        --samplers "min_p;dry;xtc;temperature"
        --min-p 0.01
        --dry-multiplier 1.1 # 1.0
        --dry-allowed-length 4  # 3
        --dry-penalty-last-n 1024
        --temp 0.6
    proxy: http://127.0.0.1:8024
    ttl: 3600


  # see https://github.com/LG-AI-EXAONE/EXAONE-Deep
  llamacpp-exaone-deep-32b:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
        --log-file /logs/llamacpp-exaone-deep-32b
        --port 8028
        --hf-repo bartowski/LGAI-EXAONE_EXAONE-Deep-32B-GGUF:Q4_K_M # 19.3GB
        --n-gpu-layers 99
        --jinja
        --ctx-size 32768
        --cache-type-k q8_0
        --cache-type-v q8_0
        --flash-attn
        --samplers "min_p;dry;xtc;temperature"
        --min-p 0.02
        --dry-multiplier 0.5
        --dry-allowed-length 3
        --dry-penalty-last-n 4096
        --temp 0.7  # 0.6 recommended, with top_p=0.95
    proxy: http://127.0.0.1:8028
    ttl: 3600
