# See: https://github.com/openai/codex/blob/main/docs/config.md#model_providers

# Recall that in TOML, root keys must be listed before tables.
model = "llamacpp-gpt-oss-120b"
model_provider = "openai-chat-completions"
model_context_window = 65536
sandbox_mode = "danger-full-access"  # we run this in an OCI-container

[model_providers.openai-chat-completions]
# Name of the provider that will be displayed in the Codex UI.
name = "llama.cpp gpt-oss-120b using Chat Completions"
# The path `/chat/completions` will be amended to this URL to make the POST
# request for the chat completions.
base_url = "http://host.docker.internal:8688/v1"
# If `env_key` is set, identifies an environment variable that must be set when
# using Codex with this provider. The value of the environment variable must be
# non-empty and will be used in the `Bearer TOKEN` HTTP header for the POST request.
env_key = "LLAMA_API_KEY"  # default: "OPENAI_API_KEY"
# Valid values for wire_api are "chat" and "responses". Defaults to "chat" if omitted.
wire_api = "chat"
# If necessary, extra query params that need to be added to the URL.
# See the Azure example below.
query_params = {}