  llamacpp-Ling-Coder-lite:
    macros:
      context_window: 16384
    cmd: |
      llama-server --log-file   /logs/llamacpp-Ling-Coder-lite.log
        --port ${PORT}
        --hf-repo mradermacher/Ling-Coder-lite-GGUF:Q4_K_M
        --n-gpu-layers 99
        --ctx-size ${context_window}
        --cache-type-k q8_0
        --cache-type-v q5_1
        --flash-attn on
        --samplers 'top_k;dry;top_p;min_p;temperature;typ_p;xtc'
        --top-k 40
        --dry-multiplier 0.5
        --dry-allowed-length 5
        --top-p 0.95
        --min-p 0.01
        --temp 0.8
    metadata:
      context_window: ${context_window}

    proxy: http://127.0.0.1:${PORT}
    ttl: 3600

  llamacpp-ling-lite-1.5:
    macros:
      context_window: 32768
    cmd: |
      llama-server --log-file   /logs/llamacpp-ling-lite-0415.log
        --port ${PORT}
        #--hf-repo bartowski/inclusionAI_Ling-lite-0415-GGUF:Q6_K_L
        --hf-repo mradermacher/Ling-lite-1.5-2507-i1-GGUF:Q6_K
        --ctx-size ${context_window}
        --n-gpu-layers 99
        --cache-type-k q8_0
        #--cache-type-v q8_0
        #--flash-attn  # <-- gibberish for large context?
        --samplers 'min_p;dry;xtc;temperature'
        --min-p 0.01
        --dry-multiplier 1.1
        # 1.0
        --dry-allowed-length 4
        # 3
        --dry-penalty-last-n 1024
        --temp 0.6
        #--verbose
    metadata:
      context_window: ${context_window}
    proxy: http://127.0.0.1:${PORT}
    ttl: 3600
