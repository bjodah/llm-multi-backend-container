logRequests: true

healthCheckTimeout: 300  # 5minutes, loading vLLM models can take a while

models:

  # the name below works with aider, thanks to:
  # https://github.com/Aider-AI/aider/blob/8f15269bd063a3c720ced514303b9efcc03fe29f/aider/models.py#L429
  llamacpp-QwQ-32B:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
           --port 8001
           --threads 16
           --metrics
           --ctx-size 29000
           --n-gpu-layers 99
           --seed "-1" 
           --prio 2 
           --temp 0.8 
           --repeat-penalty 1.1 
           --dry-multiplier 0.5 
           --min-p 0.01
           --top-k 40 
           --top-p 0.95 
           --samplers "top_k;dry;top_p;min_p;temperature;typ_p;xtc" 
           --dry-allowed-length 5
           --cache-type-k q8_0 
           --cache-type-v q4_0 
           --flash-attn
           --hf-repo unsloth/QwQ-32B-GGUF:Q4_K_M
           --hf-repo-draft bartowski/InfiniAILab_QwQ-0.5B-GGUF:IQ4_XS
           --override-kv tokenizer.ggml.bos_token_id=int:151643
           --jinja
           #--ctx-size 32768
           #mradermacher/QwQ-0.5B-GGUF:IQ4_XS
           #--verbose
           # --draft-max 16
           # --draft-min 5
           # --draft-p-min 0.5

    proxy: http://127.0.0.1:8001
    ttl: 3600

  # the name below works with aider, thanks to:
  # https://github.com/Aider-AI/aider/blob/8f15269bd063a3c720ced514303b9efcc03fe29f/aider/models.py#L418
  llamacpp-Qwen2.5-Coder-32B-Instruct:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
           --port 8002
           --threads 16
           --ctx-size 32768 
           --n-gpu-layers 999 
           --seed "-1" 
           --temp 0.3
           --repeat-penalty 1.1 
           --dry-multiplier 0.5 
           --min-p 0.01 
           --top-k 40 
           --top-p 0.95 
           --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc" 
           --cache-type-k q8_0 
           --cache-type-v q4_0 
           --flash-attn
           --hf-repo unsloth/Qwen2.5-Coder-32B-Instruct-GGUF:Q4_K_M
           --hf-repo-draft unsloth/Qwen2.5-Coder-1.5B-Instruct-GGUF:Q4_K_M
           #            --verbose
    proxy: http://127.0.0.1:8002
    ttl: 3600

  llamacpp-gemma-3-27b-it:
    # we could use Q5_K_M, if no other processes run on GPU...
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
           --port 8003
           --threads 16
           --cache-type-k q8_0
           --cache-type-v q8_0
           --flash-attn
           --gpu-layers 999
           --hf-repo unsloth/gemma-3-27b-it-GGUF:Q4_K_M
           --ctx-size 16000
           --temp 1.0
           --prio 2
           --temp 1.0
           --repeat-penalty 1.0
           --min-p 0.01
           --top-k 64
           --top-p 0.95
    proxy: http://127.0.0.1:8003
    ttl: 3600

  llamacpp-gemma-3-4b-it:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
           --port 8004
           --threads 16
           --cache-type-k q4_0
           --cache-type-v q4_0
           --flash-attn
           --gpu-layers 999
           --hf-repo unsloth/gemma-3-4b-it-GGUF:Q4_K_M
           --ctx-size 16000
           --temp 1.0
           --prio 2
           --temp 1.0
           --repeat-penalty 1.0
           --min-p 0.01
           --top-k 64
           --top-p 0.95
    proxy: http://127.0.0.1:8004
    ttl: 3600

  llamacpp-gemma-3-1b-it:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
           --port 8012
           --threads 16
           --cache-type-k q4_0
           --cache-type-v q4_0
           --flash-attn
           --gpu-layers 999
           --hf-repo unsloth/gemma-3-1b-it-GGUF:Q8_0
           --ctx-size 16000
           --temp 1.0
           --prio 2
           --temp 1.0
           --repeat-penalty 1.0
           --min-p 0.01
           --top-k 64
           --top-p 0.95
    proxy: http://127.0.0.1:8012
    ttl: 3600

  llamacpp-Mistral-Small-3.1-24B-Instruct-2503:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
           --port 8005
           --threads 16
           --cache-type-k q8_0
           --cache-type-v q8_0
           --flash-attn
           --gpu-layers 64
           --hf-repo bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF:Q6_K_L
           --hf-repo-draft bartowski/alamios_Mistral-Small-3.1-DRAFT-0.5B-GGUF:Q8_0
           --ctx-size 32000
           --temp 0.15
    proxy: http://127.0.0.1:8005
    ttl: 3600

  llamacpp-phi-4:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
           --port 8006
           --threads 16
           --ctx-size 4096
           --cache-type-k q4_0
           --cache-type-v q4_0
           --flash-attn
           --gpu-layers 64
           --hf-repo unsloth/phi-4-GGUF:Q2_K_L
           --temp 0.15
    proxy: http://127.0.0.1:8006
    ttl: 3600


  exllamav2-QwQ-32B:
    cmd: >
      python3 /opt/tabbyAPI/main.py --config /configs/tabby-config-qwq32b.yml
    proxy: http://127.0.0.1:8007
    ttl: 3600

  exllamav2-Qwen2.5-Coder-14B-Instruct:
    cmd: >
      python3 /opt/tabbyAPI/main.py --config /configs/tabby-config-qwen25-coder-14b.yml
    proxy: http://127.0.0.1:8008
    ttl: 3600

  # Not working: CUDA out of memory(!?)
  vllm-SmolLM2-1.7B-Instruct:
    cmd: >
      python3 -m vllm.entrypoints.openai.api_server
          --api-key sk-empty
          --port 8009
          --served-model-name vllm-SmolLM2-1.7B-Instruct
          --model HuggingFaceTB/SmolLM2-1.7B-Instruct
          --gpu-memory-utilization 0.5
          --max-model-len 2048
    proxy: http://127.0.0.1:8009
    ttl: 3600

  vllm-Qwen-QwQ-32B:
    cmd: >
      env VLLM_ATTENTION_BACKEND=FLASHINFER python3 -m vllm.entrypoints.openai.api_server
          --api-key sk-empty
          --port 8010
          --served-model-name vllm-Qwen-QwQ-32B
          --model Qwen/QwQ-32B-AWQ
          --trust-remote-code
          --gpu-memory-utilization 0.95
          --enable-prefix-caching
          --enable-chunked-prefill
          --max-model-len 20500
          --disable-sliding-window
          --generation-config Qwen/QwQ-32B-AWQ
          --enable-reasoning
          --reasoning-parser deepseek_r1
          --tool-call-parser hermes 
          --kv-cache-dtype fp8_e5m2
    proxy: http://127.0.0.1:8010
    ttl: 3600
    # --max-model-len 32768
    #  --kv-cache-dtype fp8_e4m3
    # --enable-auto-tool-choice

  llamacpp-Ling-Coder-lite:
    cmd: >
      /opt/llama.cpp/build/bin/llama-server
           --port 8011
           --ctx-size 32768
           --n-gpu-layers 99 
           --seed "-1" 
           --prio 2 
           --temp 0.8 
           --repeat-penalty 1.1 
           --dry-multiplier 0.5 
           --min-p 0.01 
           --top-k 40 
           --top-p 0.95 
           --samplers "top_k;dry;top_p;min_p;temperature;typ_p;xtc" 
           --dry-allowed-length 5
           --cache-type-k q8_0 
           --cache-type-v q4_0 
           --flash-attn
           --hf-repo mradermacher/Ling-Coder-lite-GGUF:Q4_K_M
           #--verbose
    proxy: http://127.0.0.1:8011
    ttl: 3600

  vllm-Qwen2.5-VL-32B:
    cmd: >
      python3 -m vllm.entrypoints.openai.api_server
          --api-key sk-empty
          --port 8013
          --served-model-name vllm-Qwen2.5-VL-32B
          --model Qwen/Qwen2.5-VL-32B-Instruct-AWQ
          --trust-remote-code
          --gpu-memory-utilization 0.97
          --enable-chunked-prefill
          --max-model-len 8192
          --kv-cache-dtype fp8_e5m2
    proxy: http://127.0.0.1:8013
    ttl: 3600
  
  vllm-Qwen2.5-VL-7B:
    cmd: >
      python3 -m vllm.entrypoints.openai.api_server
          --api-key sk-empty
          --port 8014
          --served-model-name vllm-Qwen2.5-VL-7B
          --model Qwen/Qwen2.5-VL-7B-Instruct-AWQ
          --trust-remote-code
          --gpu-memory-utilization 0.95
          --enable-chunked-prefill
          --max-model-len 32768
          --max-num-batched-tokens 32768
          --kv-cache-dtype fp8_e5m2
    proxy: http://127.0.0.1:8014
    ttl: 3600
  
